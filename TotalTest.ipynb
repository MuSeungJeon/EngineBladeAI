{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8d0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Memory: 25.76 GB\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 1: í™˜ê²½ ì„¤ì • ë° Import =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# ëª¨ë¸ import\n",
    "from models.unified.unified_model import UnifiedModel\n",
    "from models.heads.mask2former_damage_head import Mask2FormerLoss\n",
    "from utils.dataset import UnifiedDamageDataset, create_dataloaders\n",
    "from utils.evaluate import ModelEvaluator\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2bb3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: mask2former_20250911_234610\n",
      "Model Type: mask2former\n",
      "Batch Size: 2 x 2 = 4\n",
      "Blade Data: C:\\EngineBladeAI\\EngineInspectionAI_MS_back_up\\data\\blade_data\n",
      "Damage Data: C:\\EngineBladeAI\\EngineInspectionAI_MS\\data\\multilabeled_data_augmented\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 2: Mask2Former ì„¤ì • =====\n",
    "class Config:\n",
    "    # ë°ì´í„° ê²½ë¡œ - ë¶„ë¦¬ëœ ê²½ë¡œ\n",
    "    blade_data_root = Path(r'C:\\EngineBladeAI\\EngineInspectionAI_MS_back_up\\data\\blade_data')  # Head-Aìš©\n",
    "    damage_data_root = Path(r'C:\\EngineBladeAI\\EngineInspectionAI_MS\\data\\multilabeled_data_augmented')  # Head-Bìš©\n",
    "    \n",
    "    blade_checkpoint = 'best_unified_blade_model.pth'\n",
    "    \n",
    "    # ëª¨ë¸ íƒ€ìž…\n",
    "    model_type = 'mask2former'\n",
    "    \n",
    "    # ëª¨ë¸ ê¸°ë³¸ ì„¤ì •\n",
    "    backbone_type = 'tiny'\n",
    "    use_fpn = True\n",
    "    num_blade_classes = 2\n",
    "    num_damage_classes = 3\n",
    "    \n",
    "    # Mask2Former íŠ¹í™” ì„¤ì •\n",
    "    batch_size = 2  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    accumulate_grad_batches = 2  # Gradient accumulation\n",
    "    num_workers = 0\n",
    "    \n",
    "    # Mask2Former Head ì„¤ì •\n",
    "    mask2former_config = {\n",
    "        'num_queries': 100,  # ì²˜ìŒì—” ì ê²Œ\n",
    "        'hidden_dim': 256,\n",
    "        'num_heads': 8,\n",
    "        'dec_layers': 3,  # ì²˜ìŒì—” ì ì€ ë ˆì´ì–´\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì •\n",
    "    epochs = 30\n",
    "    learning_rate = 1e-5  # Mask2FormerëŠ” ìž‘ì€ lr\n",
    "    weight_decay = 0.05\n",
    "    gradient_clip = 0.01  # ìž‘ì€ gradient clipping\n",
    "    \n",
    "    # Mixed Precision Training\n",
    "    use_amp = True\n",
    "    \n",
    "    # í•™ìŠµ ì „ëžµ\n",
    "    freeze_blade_initially = True\n",
    "    unfreeze_epoch = 15\n",
    "    \n",
    "    # Loss weights\n",
    "    blade_loss_weight = 1.0\n",
    "    aux_loss_weight = 0.4\n",
    "    \n",
    "    # ê¸°íƒ€\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    save_dir = Path('outputs_mask2former')\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    experiment_name = f\"mask2former_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "config = Config()\n",
    "print(f\"Experiment: {config.experiment_name}\")\n",
    "print(f\"Model Type: {config.model_type}\")\n",
    "print(f\"Batch Size: {config.batch_size} x {config.accumulate_grad_batches} = {config.batch_size * config.accumulate_grad_batches}\")\n",
    "print(f\"Blade Data: {config.blade_data_root}\")\n",
    "print(f\"Damage Data: {config.damage_data_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0755ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...\n",
      "âœ… Train: 2352 batches\n",
      "âœ… Valid: 453 batches\n",
      "âœ… Test: 460 batches\n",
      "\n",
      "ë°ì´í„° ìƒ˜í”Œ:\n",
      "  image: torch.Size([2, 3, 640, 640])\n",
      "  blade_mask: torch.Size([2, 640, 640])\n",
      "  multilabel: torch.Size([2, 3])\n",
      "  instance_masks: 2 items\n",
      "  instance_labels: 2 items\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 3: ë°ì´í„°ë¡œë” ìƒì„± =====\n",
    "print(\"ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...\")\n",
    "\n",
    "train_loader, valid_loader, test_loader = create_dataloaders(\n",
    "    blade_data_root=config.blade_data_root,\n",
    "    damage_data_root=config.damage_data_root,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    model_type='mask2former'\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train: {len(train_loader)} batches\")\n",
    "print(f\"âœ… Valid: {len(valid_loader)} batches\")\n",
    "print(f\"âœ… Test: {len(test_loader)} batches\")\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "for batch in train_loader:\n",
    "    print(f\"\\në°ì´í„° ìƒ˜í”Œ:\")\n",
    "    for key, value in batch.items():\n",
    "        if torch.is_tensor(value):\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"  {key}: {len(value)} items\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc6d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask2Former ëª¨ë¸ ìƒì„± ì¤‘...\n",
      "âœ… Total parameters: 50.31M\n",
      "âœ… Trainable parameters: 49.78M\n",
      "âœ… Blade head frozen: True\n",
      "âœ… Damage head type: Mask2Former\n",
      "  - Queries: 100\n",
      "  - Decoder layers: 3\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 4: Mask2Former ëª¨ë¸ ìƒì„± =====\n",
    "print(\"Mask2Former ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "\n",
    "model = UnifiedModel(\n",
    "    backbone_type=config.backbone_type,\n",
    "    num_blade_classes=config.num_blade_classes,\n",
    "    num_damage_classes=config.num_damage_classes,\n",
    "    pretrained_backbone=True,\n",
    "    blade_checkpoint=config.blade_checkpoint if Path(config.blade_checkpoint).exists() else None,\n",
    "    freeze_blade=config.freeze_blade_initially,\n",
    "    use_fpn=config.use_fpn,\n",
    "    damage_head_type='mask2former',\n",
    "    damage_head_config=config.mask2former_config\n",
    ")\n",
    "\n",
    "model = model.to(config.device)\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… Total parameters: {total_params/1e6:.2f}M\")\n",
    "print(f\"âœ… Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "print(f\"âœ… Blade head frozen: {config.freeze_blade_initially}\")\n",
    "print(f\"âœ… Damage head type: Mask2Former\")\n",
    "print(f\"  - Queries: {config.mask2former_config['num_queries']}\")\n",
    "print(f\"  - Decoder layers: {config.mask2former_config['dec_layers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb73a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loss ready (BCEWithLogitsLoss)\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 5 ì™„ì „ êµì²´: SimpleLoss ì‚¬ìš© =====\n",
    "class SimpleLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.blade_ce = nn.CrossEntropyLoss()\n",
    "        self.ml_loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, outputs, batch):\n",
    "        losses = {}\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Blade loss\n",
    "        if 'blade' in outputs and 'blade_mask' in batch:\n",
    "            losses['blade'] = self.blade_ce(outputs['blade'], batch['blade_mask'])\n",
    "            total_loss += losses['blade'] * self.config.blade_loss_weight\n",
    "        \n",
    "        # Multilabel loss\n",
    "        if 'multilabel' in outputs and 'multilabel' in batch:\n",
    "            # Check if outputs need sigmoid\n",
    "            if outputs['multilabel'].max() > 1.0 or outputs['multilabel'].min() < 0:\n",
    "                # Use BCEWithLogitsLoss for raw logits\n",
    "                losses['ml'] = self.ml_loss(outputs['multilabel'], batch['multilabel'])\n",
    "            else:\n",
    "                # Already sigmoid applied\n",
    "                losses['ml'] = F.binary_cross_entropy(outputs['multilabel'], batch['multilabel'])\n",
    "            total_loss += losses['ml'] * 2.0\n",
    "        \n",
    "        losses['total'] = total_loss\n",
    "        return total_loss, losses\n",
    "\n",
    "criterion = SimpleLoss(config)\n",
    "print(\"âœ… SimpleLoss ready - no Hungarian matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab67d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Optimizer ready with 2 groups\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 6: Optimizer =====\n",
    "param_groups = [\n",
    "    {'params': model.backbone.parameters(), 'lr': config.learning_rate * 0.1, 'name': 'backbone'}\n",
    "]\n",
    "\n",
    "# Damage head parameters\n",
    "for name, param in model.damage_head.named_parameters():\n",
    "    param_groups.append({'params': [param], 'lr': config.learning_rate, 'name': 'damage_head'})\n",
    "\n",
    "if not config.freeze_blade_initially:\n",
    "    param_groups.append({'params': model.blade_head.parameters(), 'lr': config.learning_rate * 0.5, 'name': 'blade_head'})\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups[:2], weight_decay=config.weight_decay)  # ì²˜ìŒ 2ê°œ ê·¸ë£¹ë§Œ\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=1e-7)\n",
    "scaler = GradScaler() if config.use_amp else None\n",
    "\n",
    "print(f\"âœ… Optimizer ready with {len(param_groups[:2])} groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfacb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training functions ready\n"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 7: Training Functions =====\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, config, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs}')\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        for key in batch:\n",
    "            if torch.is_tensor(batch[key]):\n",
    "                batch[key] = batch[key].to(config.device)\n",
    "            elif isinstance(batch[key], list):\n",
    "                batch[key] = [item.to(config.device) if torch.is_tensor(item) else item for item in batch[key]]\n",
    "        \n",
    "        with autocast(enabled=config.use_amp):\n",
    "            outputs = model(batch['image'])\n",
    "            loss, loss_dict = criterion(outputs, batch)\n",
    "            loss = loss / config.accumulate_grad_batches\n",
    "        \n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % config.accumulate_grad_batches == 0:\n",
    "            if scaler:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * config.accumulate_grad_batches\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f\"{loss.item() * config.accumulate_grad_batches:.4f}\"})\n",
    "    \n",
    "    scheduler.step()\n",
    "    return {'total': total_loss / num_batches}\n",
    "\n",
    "def validate_epoch(model, valid_loader, criterion, config):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc='Validation'):\n",
    "            for key in batch:\n",
    "                if torch.is_tensor(batch[key]):\n",
    "                    batch[key] = batch[key].to(config.device)\n",
    "                elif isinstance(batch[key], list):\n",
    "                    batch[key] = [item.to(config.device) if torch.is_tensor(item) else item for item in batch[key]]\n",
    "            \n",
    "            outputs = model(batch['image'])\n",
    "            loss, _ = criterion(outputs, batch)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return {'loss': total_loss / num_batches}\n",
    "\n",
    "print(\"âœ… Training functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cd3adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Mask2Former í•™ìŠµ ì‹œìž‘\n",
      "============================================================\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/2352 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x640000 and 6400x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m         param.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m     optimizer.add_param_group({\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: model.blade_head.parameters(), \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: config.learning_rate * \u001b[32m0.1\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m train_metrics = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m val_metrics = validate_epoch(model, valid_loader, criterion, config)\n\u001b[32m     21\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_metrics[\u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, scheduler, scaler, config, epoch)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled=config.use_amp):\n\u001b[32m     17\u001b[39m     outputs = model(batch[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     loss, loss_dict = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     loss = loss / config.accumulate_grad_batches\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scaler:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mUnifiedMask2FormerLoss.forward\u001b[39m\u001b[34m(self, outputs, batch)\u001b[39m\n\u001b[32m     40\u001b[39m mask2former_targets = {\n\u001b[32m     41\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minstance_masks\u001b[39m\u001b[33m'\u001b[39m: batch.get(\u001b[33m'\u001b[39m\u001b[33minstance_masks\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     42\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minstance_labels\u001b[39m\u001b[33m'\u001b[39m: batch.get(\u001b[33m'\u001b[39m\u001b[33minstance_labels\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmultilabel\u001b[39m\u001b[33m'\u001b[39m: batch[\u001b[33m'\u001b[39m\u001b[33mmultilabel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     44\u001b[39m }\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask2former_outputs[\u001b[33m'\u001b[39m\u001b[33mpred_logits\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     damage_loss, damage_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask2former_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask2former_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask2former_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m damage_losses.items():\n\u001b[32m     49\u001b[39m         losses[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdamage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] = value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\EngineBladeAI\\EngineInspectionAI_MS\\models\\heads\\mask2former_damage_head.py:415\u001b[39m, in \u001b[36mMask2FormerLoss.forward\u001b[39m\u001b[34m(self, outputs, targets)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;66;03m# Instance matching and losses\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mpred_logits\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mpred_masks\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[32m    414\u001b[39m     \u001b[38;5;66;03m# Get matched indices\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     indices = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     \u001b[38;5;66;03m# Classification loss\u001b[39;00m\n\u001b[32m    418\u001b[39m     losses[\u001b[33m'\u001b[39m\u001b[33mce\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_loss_ce(outputs, targets, indices)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\EngineBladeAI\\EngineInspectionAI_MS\\models\\heads\\mask2former_damage_head.py:610\u001b[39m, in \u001b[36mHungarianMatcher.forward\u001b[39m\u001b[34m(self, outputs, targets)\u001b[39m\n\u001b[32m    607\u001b[39m tgt_masks_flat = tgt_masks.flatten(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [N, H*W]\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# Cost ê³„ì‚°\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m cost_mask = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_masks_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_masks_flat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[38;5;66;03m# ê°„ë‹¨í•œ ë§¤ì¹­\u001b[39;00m\n\u001b[32m    613\u001b[39m C = cost_mask.cpu().numpy()\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (2x640000 and 6400x1)"
     ]
    }
   ],
   "source": [
    "# ===== ì…€ 8: Training Loop =====\n",
    "print(\"=\"*60)\n",
    "print(\"Mask2Former í•™ìŠµ ì‹œìž‘\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n",
    "    \n",
    "    if epoch == config.unfreeze_epoch and config.freeze_blade_initially:\n",
    "        print(\"ðŸ”“ Unfreezing Blade Head\")\n",
    "        for param in model.blade_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer.add_param_group({'params': model.blade_head.parameters(), 'lr': config.learning_rate * 0.1})\n",
    "    \n",
    "    train_metrics = train_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, config, epoch)\n",
    "    val_metrics = validate_epoch(model, valid_loader, criterion, config)\n",
    "    \n",
    "    history['train_loss'].append(train_metrics['total'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    \n",
    "    print(f\"Train Loss: {train_metrics['total']:.4f}, Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    \n",
    "    if val_metrics['loss'] < best_loss:\n",
    "        best_loss = val_metrics['loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss\n",
    "        }, config.save_dir / f'{config.experiment_name}_best.pth')\n",
    "        print(f\"âœ… Best model saved!\")\n",
    "\n",
    "print(f\"\\ní•™ìŠµ ì™„ë£Œ! Best loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac6dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
