{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# --- 프로젝트 경로 추가 및 모듈 임포트 ---\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# pycocotools와 torchmetrics가 필요합니다.\n",
    "# pip install pycocotools torchmetrics\n",
    "from pycocotools.coco import COCO\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "from models.blade_model_v2 import BladeModelV2\n",
    "from utils.criterion import SetCriterion\n",
    "from utils.hungarian_matcher import HungarianMatcher\n",
    "\n",
    "# --- COCO 2017 Validation 데이터셋 자동 다운로드 ---\n",
    "print(\"--- Setting up COCO 2017 Validation Dataset ---\")\n",
    "data_dir = Path('./data/coco')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Annotations\n",
    "if not (data_dir / 'annotations' / 'instances_val2017.json').exists():\n",
    "    print(\"Downloading annotations...\")\n",
    "    url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    urllib.request.urlretrieve(url, data_dir / 'annotations.zip')\n",
    "    with zipfile.ZipFile(data_dir / 'annotations.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_dir / 'annotations.zip')\n",
    "    print(\"Annotations downloaded and extracted.\")\n",
    "\n",
    "# Images\n",
    "if not (data_dir / 'val2017').exists() or len(os.listdir(data_dir / 'val2017')) == 0:\n",
    "    print(\"Downloading images (approx. 1GB)... This may take a while.\")\n",
    "    url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "    urllib.request.urlretrieve(url, data_dir / 'val2017.zip')\n",
    "    with zipfile.ZipFile(data_dir / 'val2017.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_dir / 'val2017.zip')\n",
    "    print(\"Images downloaded and extracted.\")\n",
    "\n",
    "print(\"✅ COCO Dataset is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352230c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 셀 2: COCO 테스트용 설정 (수정) =====\n",
    "\n",
    "class CocoConfig:\n",
    "    DATA_ROOT = Path('./data/coco')\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 2\n",
    "    LR = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "    # --- [핵심 수정] ---\n",
    "    # 딕셔너리를 SimpleNamespace로 변경하여 점(.)으로 접근 가능하게 수정\n",
    "    MODEL = SimpleNamespace(\n",
    "        BACKBONE=SimpleNamespace(NAME='ConvNeXt-Tiny'),\n",
    "        FPN=SimpleNamespace(OUT_CHANNELS=256),\n",
    "        HEAD_B=SimpleNamespace(\n",
    "            FEAT_CHANNELS=256,\n",
    "            OUT_CHANNELS=256,\n",
    "            NUM_CLASSES=80,\n",
    "            QUERIES_PER_CLASS=5,\n",
    "            DEC_LAYERS=6\n",
    "        )\n",
    "    )\n",
    "    LOSS = SimpleNamespace(\n",
    "        CLASS_WEIGHTS=[1.0] * 80,\n",
    "        EOS_COEF=0.1\n",
    "    )\n",
    "\n",
    "config = CocoConfig()\n",
    "print(\"--- COCO Test Configuration Initialized ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, split='val', transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.images_dir = self.root / f\"{self.split}2017\"\n",
    "        self.coco = COCO(self.root / 'annotations' / f\"instances_{self.split}2017.json\")\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        # COCO의 91개 카테고리를 0-79 인덱스로 매핑\n",
    "        self.cat_ids = sorted(self.coco.getCatIds())\n",
    "        self.cat2cat = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((640, 640)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = self.images_dir / img_info['file_name']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        target = {}\n",
    "        masks = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            if ann['iscrowd'] == 0:\n",
    "                masks.append(self.coco.annToMask(ann))\n",
    "                labels.append(self.cat2cat[ann['category_id']])\n",
    "\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        if masks:\n",
    "            target['masks'] = torch.from_numpy(np.stack(masks)).float()\n",
    "        else:\n",
    "            target['masks'] = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.float32)\n",
    "            \n",
    "        # 우리 모델과의 호환성을 위해 blade_mask 필드 추가 (여기서는 사용되지 않음)\n",
    "        target['blade_mask'] = torch.zeros(image.shape[1:], dtype=torch.long)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "# COCO val 데이터셋을 train/val로 나눠서 테스트\n",
    "full_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "train_size = int(0.1 * len(full_dataset)) # 10%만 빠르게 테스트\n",
    "val_size = int(0.02 * len(full_dataset)) # 2%만 검증\n",
    "train_subset, val_subset, _ = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, len(full_dataset) - train_size - val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "print(\"✅ COCO DataLoaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a77dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화 (Head-A는 사용하지 않으므로, 손실 계산 시 무시)\n",
    "model = BladeModelV2(config).to(config.DEVICE)\n",
    "matcher = HungarianMatcher(cost_class=2.0, cost_mask=5.0, cost_dice=5.0)\n",
    "weight_dict = {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0}\n",
    "criterion = SetCriterion(\n",
    "    num_classes=config.MODEL.HEAD_B.NUM_CLASSES, matcher=matcher, weight_dict=weight_dict,\n",
    "    eos_coef=config.LOSS.EOS_COEF, losses=['labels', 'masks'],\n",
    "    class_weights=config.LOSS.CLASS_WEIGHTS\n",
    ").to(config.DEVICE)\n",
    "optimizer = AdamW(model.head_b.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY) # Head-B만 학습\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_epoch_coco(model, criterion, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    model.head_b.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"COCO Test Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss_dict = criterion(outputs, targets_gpu)\n",
    "            weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(weighted_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += weighted_loss.item()\n",
    "        pbar.set_postfix({'loss': f'{weighted_loss.item():.4f}'})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_coco(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar:\n",
    "            images = images.to(device)\n",
    "            targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss_dict = criterion(outputs, targets_gpu)\n",
    "                weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "            val_losses.append(weighted_loss.item())\n",
    "\n",
    "            # --- [수정] 이 부분이 생략되었던 곳입니다 ---\n",
    "            # 모델의 GPU 출력값을 CPU로 가져옴\n",
    "            pred_logits_cpu = outputs['pred_logits'].cpu()\n",
    "            pred_masks_cpu = outputs['pred_masks'].cpu()\n",
    "            \n",
    "            # 1. Prediction을 torchmetrics 형식으로 변환\n",
    "            preds_for_map = []\n",
    "            for i in range(len(targets)):\n",
    "                scores, labels = F.softmax(pred_logits_cpu[i], dim=-1).max(-1)\n",
    "                masks_bool = (torch.sigmoid(pred_masks_cpu[i]) > 0.5)\n",
    "                preds_for_map.append(dict(\n",
    "                    masks=masks_bool,\n",
    "                    scores=scores,\n",
    "                    labels=labels,\n",
    "                ))\n",
    "\n",
    "            # 2. Target을 torchmetrics 형식으로 변환\n",
    "            targets_for_map = []\n",
    "            for t in targets:\n",
    "                # CPU로 옮길 필요 없음 (targets는 이미 CPU에 있음)\n",
    "                targets_for_map.append(dict(\n",
    "                    masks=(t['masks'] > 0.5), # boolean mask로 변환\n",
    "                    labels=t['labels'],\n",
    "                ))\n",
    "            \n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "            \n",
    "    map_results = map_metric.compute()\n",
    "    return {'loss': np.mean(val_losses), 'mAP': map_results['map'].item()}\n",
    "\n",
    "print(\"✅ COCO Test functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 🚀 Starting COCO Sanity Check Training 🚀 ---\")\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_loss = train_epoch_coco(model, criterion, train_loader, optimizer, config.DEVICE, epoch)\n",
    "    val_metrics = validate_coco(model, criterion, val_loader, config.DEVICE)\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS} -> Train Loss: {train_loss:.4f}, Val Loss: {val_metrics['loss']:.4f}, mAP: {val_metrics['mAP']:.4f}\")\n",
    "\n",
    "print(\"\\n--- 🎉 COCO Test Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f8a59",
   "metadata": {},
   "source": [
    "Mask R CNN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "✅ COCO DataLoaders created!\n",
      "--- Mask R-CNN Test Configuration ---\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# --- pycocotools와 torchmetrics가 필요합니다 ---\n",
    "from pycocotools.coco import COCO\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, split='val'):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.images_dir = self.root / f\"{self.split}2017\"\n",
    "        self.coco = COCO(self.root / 'annotations' / f\"instances_{self.split}2017.json\")\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        self.cat_ids = sorted(self.coco.getCatIds())\n",
    "        self.coco_labels_map = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        coco_anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = self.images_dir / img_info['file_name']\n",
    "        \n",
    "        # --- [수정] 이미지를 PIL Image 객체로 그대로 반환 (transform 제거) ---\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        target = {}\n",
    "        masks, labels, boxes = [], [], []\n",
    "\n",
    "        for ann in coco_anns:\n",
    "            if ann['iscrowd'] == 0 and ann['area'] > 0:\n",
    "                masks.append(self.coco.annToMask(ann))\n",
    "                labels.append(self.coco_labels_map[ann['category_id']])\n",
    "                x, y, w, h = ann['bbox']\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "        \n",
    "        if boxes:\n",
    "            target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            target['labels'] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            target['masks'] = torch.as_tensor(np.stack(masks), dtype=torch.uint8)\n",
    "        else: \n",
    "            target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "            target['masks'] = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "# --- [수정] collate_fn에서 ToTensor 변환을 담당 ---\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    for item in batch:\n",
    "        # 각 PIL 이미지를 텐서로 변환하여 리스트에 추가\n",
    "        images.append(to_tensor(item[0]))\n",
    "        targets.append(item[1])\n",
    "    return images, targets\n",
    "\n",
    "# COCO val 데이터셋을 train/val로 나눠서 테스트\n",
    "full_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "train_size = int(0.1 * len(full_dataset)) # 10%만 빠르게 테스트\n",
    "val_size = int(0.02 * len(full_dataset)) # 2%만 검증\n",
    "train_subset, val_subset, _ = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, len(full_dataset) - train_size - val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "print(\"✅ COCO DataLoaders created!\")\n",
    "\n",
    "class CocoConfig:\n",
    "    DATA_ROOT = Path('./data/coco')\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 2\n",
    "    LR = 5e-4 # Mask R-CNN은 약간 더 높은 LR을 사용합니다.\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "config = CocoConfig()\n",
    "print(\"--- Mask R-CNN Test Configuration ---\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4491bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n",
      "✅ COCO DataLoaders created!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. 데이터 로더 생성 (이전과 동일)\n",
    "# ==============================================================================\n",
    "full_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "train_size = int(0.1 * len(full_dataset))\n",
    "val_size = int(0.02 * len(full_dataset))\n",
    "train_subset, val_subset, _ = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, len(full_dataset) - train_size - val_size]\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "print(\"✅ COCO DataLoaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1460ced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mask R-CNN model created!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. Mask R-CNN 모델 생성\n",
    "# ==============================================================================\n",
    "def get_maskrcnn_model(num_classes):\n",
    "    # COCO에서 사전 학습된 모델 불러오기\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # 분류기(classifier)를 새로운 클래스 수에 맞게 교체\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # 마스크 예측기(mask predictor)를 새로운 클래스 수에 맞게 교체\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# COCO는 80개 클래스 + 배경(background) 1개 = 총 81개\n",
    "model = get_maskrcnn_model(81).to(config.DEVICE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=config.LR)\n",
    "print(\"✅ Mask R-CNN model created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a179671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ COCO Test functions are ready.\n"
     ]
    }
   ],
   "source": [
    "def train_epoch_coco(model, criterion, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    model.head_b.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"COCO Test Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss_dict = criterion(outputs, targets_gpu)\n",
    "            weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(weighted_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += weighted_loss.item()\n",
    "        pbar.set_postfix({'loss': f'{weighted_loss.item():.4f}'})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_coco(model, dataloader, device):\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(dataloader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar_val:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            \n",
    "            # --- [핵심 수정] torchmetrics 형식 변환 시 uint8 사용 ---\n",
    "            preds_for_map = []\n",
    "            for p in predictions:\n",
    "                p_cpu = {k: v.cpu() for k, v in p.items()}\n",
    "                # 예측된 마스크를 thresholding 후 uint8로 변환\n",
    "                p_cpu['masks'] = (p_cpu['masks'].squeeze(1) > 0.5).to(torch.uint8)\n",
    "                preds_for_map.append(p_cpu)\n",
    "\n",
    "            targets_for_map = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "            \n",
    "    map_results = map_metric.compute()\n",
    "    print(f\"\\n -> Val mAP: {map_results['map'].item():.4f}\")\n",
    "\n",
    "print(\"✅ COCO Test functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53de8802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 🚀 Starting Mask R-CNN Sanity Check Training 🚀 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mask R-CNN Epoch 1/2 [Train]:   0%|          | 0/125 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMask R-CNN Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Train]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- 학습 ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1175\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mcollate_fn\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     74\u001b[39m to_tensor = transforms.ToTensor()\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# 각 PIL 이미지를 텐서로 변환하여 리스트에 추가\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     images.append(\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     78\u001b[39m     targets.append(item[\u001b[32m1\u001b[39m])\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m images, targets\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:142\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    140\u001b[39m     _log_api_usage_once(to_tensor)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil._is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. Mask R-CNN 학습 루프\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 🚀 Starting Mask R-CNN Sanity Check Training 🚀 ---\")\n",
    "for epoch in range(config.EPOCHS):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Mask R-CNN Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    \n",
    "    # --- 학습 ---\n",
    "    for images, targets in pbar:\n",
    "        images = list(img.to(config.DEVICE) for img in images)\n",
    "        targets = [{k: v.to(config.DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Mask R-CNN은 학습 시 loss 딕셔너리를 직접 반환\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{losses.item():.4f}'})\n",
    "\n",
    "    # --- 검증 ---\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(val_loader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar_val:\n",
    "            images = list(img.to(config.DEVICE) for img in images)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            \n",
    "            # torchmetrics 형식에 맞게 변환\n",
    "            preds_for_map = [{k: v.cpu() for k, v in p.items()} for p in predictions]\n",
    "            targets_for_map = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "            \n",
    "    map_results = map_metric.compute()\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS} -> mAP: {map_results['map'].item():.4f}\")\n",
    "\n",
    "print(\"\\n--- 🎉 Mask R-CNN Test Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3639b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 🚀 Starting Final Pre-flight Check... ---\n",
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "✅ Dataset and DataLoader are OK.\n",
      "✅ Model initialization is OK.\n",
      "✅ Training forward pass is OK (Loss: 5.8424).\n",
      "✅ Validation logic and mAP calculation is OK (Sample mAP: 0.0000).\n",
      "\n",
      "\n",
      "✅✅✅ Pre-flight Check PASSED! All components are working correctly. ✅✅✅\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# pycocotools와 torchmetrics가 필요합니다.\n",
    "from pycocotools.coco import COCO\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "print(\"--- 🚀 Starting Final Pre-flight Check... ---\")\n",
    "\n",
    "try:\n",
    "    # ----------------------------------------------------\n",
    "    # 1. 설정 (Configuration)\n",
    "    # ----------------------------------------------------\n",
    "    class CocoConfig:\n",
    "        DATA_ROOT = Path('./data/coco')\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        BATCH_SIZE = 2 # 테스트를 위해 작은 배치 크기 사용\n",
    "        NUM_WORKERS = 0\n",
    "\n",
    "    config = CocoConfig()\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2. 데이터셋 클래스 및 Collate 함수 (모든 수정사항 반영)\n",
    "    # ----------------------------------------------------\n",
    "    class CocoDataset(Dataset):\n",
    "        def __init__(self, root, split='val'):\n",
    "            self.root = Path(root)\n",
    "            self.split = split\n",
    "            self.images_dir = self.root / f\"{split}2017\"\n",
    "            self.coco = COCO(self.root / 'annotations' / f\"instances_{split}2017.json\")\n",
    "            self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "            self.cat_ids = sorted(self.coco.getCatIds())\n",
    "            self.coco_labels_map = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}\n",
    "            self.transform = transforms.ToTensor()\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_id = self.ids[idx]\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            coco_anns = self.coco.loadAnns(ann_ids)\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            image = Image.open(self.images_dir / img_info['file_name']).convert('RGB')\n",
    "            \n",
    "            target = {}\n",
    "            masks, labels, boxes = [], [], []\n",
    "            for ann in coco_anns:\n",
    "                if ann['iscrowd'] == 0 and ann['area'] > 0:\n",
    "                    masks.append(self.coco.annToMask(ann))\n",
    "                    labels.append(self.coco_labels_map[ann['category_id']])\n",
    "                    x, y, w, h = ann['bbox']\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "            \n",
    "            image = self.transform(image)\n",
    "            \n",
    "            if boxes:\n",
    "                target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                target['labels'] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "                target['masks'] = torch.as_tensor(np.stack(masks), dtype=torch.uint8)\n",
    "            else: \n",
    "                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "                target['masks'] = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "            return image, target\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3. 데이터 로더 생성\n",
    "    # ----------------------------------------------------\n",
    "    val_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, collate_fn=collate_fn)\n",
    "    \n",
    "    print(\"✅ Dataset and DataLoader are OK.\")\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # 4. 모델 초기화\n",
    "    # ----------------------------------------------------\n",
    "    def get_maskrcnn_model(num_classes):\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "        return model\n",
    "\n",
    "    model = get_maskrcnn_model(81).to(config.DEVICE)\n",
    "    print(\"✅ Model initialization is OK.\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 5. 최종 검증 로직 테스트\n",
    "    # ----------------------------------------------------\n",
    "    images, targets = next(iter(val_loader))\n",
    "    \n",
    "    # 학습 모드 테스트\n",
    "    model.train()\n",
    "    images_gpu = list(img.to(config.DEVICE) for img in images)\n",
    "    targets_gpu = [{k: v.to(config.DEVICE) for k, v in t.items()} for t in targets]\n",
    "    loss_dict = model(images_gpu, targets_gpu)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    print(f\"✅ Training forward pass is OK (Loss: {losses.item():.4f}).\")\n",
    "\n",
    "    # 검증 모드 테스트\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images_gpu)\n",
    "        preds_for_map = []\n",
    "        for p in predictions:\n",
    "            p_cpu = {k: v.cpu() for k, v in p.items()}\n",
    "            p_cpu['masks'] = (p_cpu['masks'].squeeze(1) > 0.5).to(torch.uint8)\n",
    "            preds_for_map.append(p_cpu)\n",
    "        targets_for_map = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "        map_metric.update(preds_for_map, targets_for_map)\n",
    "        map_results = map_metric.compute()\n",
    "\n",
    "    print(f\"✅ Validation logic and mAP calculation is OK (Sample mAP: {map_results['map']:.4f}).\")\n",
    "    \n",
    "    print(\"\\n\\n✅✅✅ Pre-flight Check PASSED! All components are working correctly. ✅✅✅\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n❌❌❌ Pre-flight Check FAILED! An error occurred. ❌❌❌\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de976b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd855c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
