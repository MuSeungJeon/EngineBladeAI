{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# --- ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä Î∞è Î™®Îìà ÏûÑÌè¨Ìä∏ ---\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# pycocotoolsÏôÄ torchmetricsÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.\n",
    "# pip install pycocotools torchmetrics\n",
    "from pycocotools.coco import COCO\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "from models.blade_model_v2 import BladeModelV2\n",
    "from utils.criterion import SetCriterion\n",
    "from utils.hungarian_matcher import HungarianMatcher\n",
    "\n",
    "# --- COCO 2017 Validation Îç∞Ïù¥ÌÑ∞ÏÖã ÏûêÎèô Îã§Ïö¥Î°úÎìú ---\n",
    "print(\"--- Setting up COCO 2017 Validation Dataset ---\")\n",
    "data_dir = Path('./data/coco')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Annotations\n",
    "if not (data_dir / 'annotations' / 'instances_val2017.json').exists():\n",
    "    print(\"Downloading annotations...\")\n",
    "    url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    urllib.request.urlretrieve(url, data_dir / 'annotations.zip')\n",
    "    with zipfile.ZipFile(data_dir / 'annotations.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_dir / 'annotations.zip')\n",
    "    print(\"Annotations downloaded and extracted.\")\n",
    "\n",
    "# Images\n",
    "if not (data_dir / 'val2017').exists() or len(os.listdir(data_dir / 'val2017')) == 0:\n",
    "    print(\"Downloading images (approx. 1GB)... This may take a while.\")\n",
    "    url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "    urllib.request.urlretrieve(url, data_dir / 'val2017.zip')\n",
    "    with zipfile.ZipFile(data_dir / 'val2017.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_dir / 'val2017.zip')\n",
    "    print(\"Images downloaded and extracted.\")\n",
    "\n",
    "print(\"‚úÖ COCO Dataset is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352230c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ÏÖÄ 2: COCO ÌÖåÏä§Ìä∏Ïö© ÏÑ§Ï†ï (ÏàòÏ†ï) =====\n",
    "\n",
    "class CocoConfig:\n",
    "    DATA_ROOT = Path('./data/coco')\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 2\n",
    "    LR = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "    # --- [ÌïµÏã¨ ÏàòÏ†ï] ---\n",
    "    # ÎîïÏÖîÎÑàÎ¶¨Î•º SimpleNamespaceÎ°ú Î≥ÄÍ≤ΩÌïòÏó¨ Ï†ê(.)ÏúºÎ°ú Ï†ëÍ∑º Í∞ÄÎä•ÌïòÍ≤å ÏàòÏ†ï\n",
    "    MODEL = SimpleNamespace(\n",
    "        BACKBONE=SimpleNamespace(NAME='ConvNeXt-Tiny'),\n",
    "        FPN=SimpleNamespace(OUT_CHANNELS=256),\n",
    "        HEAD_B=SimpleNamespace(\n",
    "            FEAT_CHANNELS=256,\n",
    "            OUT_CHANNELS=256,\n",
    "            NUM_CLASSES=80,\n",
    "            QUERIES_PER_CLASS=5,\n",
    "            DEC_LAYERS=6\n",
    "        )\n",
    "    )\n",
    "    LOSS = SimpleNamespace(\n",
    "        CLASS_WEIGHTS=[1.0] * 80,\n",
    "        EOS_COEF=0.1\n",
    "    )\n",
    "\n",
    "config = CocoConfig()\n",
    "print(\"--- COCO Test Configuration Initialized ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, split='val', transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.images_dir = self.root / f\"{self.split}2017\"\n",
    "        self.coco = COCO(self.root / 'annotations' / f\"instances_{self.split}2017.json\")\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        # COCOÏùò 91Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨Î•º 0-79 Ïù∏Îç±Ïä§Î°ú Îß§Ìïë\n",
    "        self.cat_ids = sorted(self.coco.getCatIds())\n",
    "        self.cat2cat = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((640, 640)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = self.images_dir / img_info['file_name']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        target = {}\n",
    "        masks = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            if ann['iscrowd'] == 0:\n",
    "                masks.append(self.coco.annToMask(ann))\n",
    "                labels.append(self.cat2cat[ann['category_id']])\n",
    "\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        if masks:\n",
    "            target['masks'] = torch.from_numpy(np.stack(masks)).float()\n",
    "        else:\n",
    "            target['masks'] = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.float32)\n",
    "            \n",
    "        # Ïö∞Î¶¨ Î™®Îç∏Í≥ºÏùò Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥ blade_mask ÌïÑÎìú Ï∂îÍ∞Ä (Ïó¨Í∏∞ÏÑúÎäî ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÏùå)\n",
    "        target['blade_mask'] = torch.zeros(image.shape[1:], dtype=torch.long)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "# COCO val Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ train/valÎ°ú ÎÇòÎà†ÏÑú ÌÖåÏä§Ìä∏\n",
    "full_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "train_size = int(0.1 * len(full_dataset)) # 10%Îßå Îπ†Î•¥Í≤å ÌÖåÏä§Ìä∏\n",
    "val_size = int(0.02 * len(full_dataset)) # 2%Îßå Í≤ÄÏ¶ù\n",
    "train_subset, val_subset, _ = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, len(full_dataset) - train_size - val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "print(\"‚úÖ COCO DataLoaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a77dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî (Head-AÎäî ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏúºÎØÄÎ°ú, ÏÜêÏã§ Í≥ÑÏÇ∞ Ïãú Î¨¥Ïãú)\n",
    "model = BladeModelV2(config).to(config.DEVICE)\n",
    "matcher = HungarianMatcher(cost_class=2.0, cost_mask=5.0, cost_dice=5.0)\n",
    "weight_dict = {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0}\n",
    "criterion = SetCriterion(\n",
    "    num_classes=config.MODEL.HEAD_B.NUM_CLASSES, matcher=matcher, weight_dict=weight_dict,\n",
    "    eos_coef=config.LOSS.EOS_COEF, losses=['labels', 'masks'],\n",
    "    class_weights=config.LOSS.CLASS_WEIGHTS\n",
    ").to(config.DEVICE)\n",
    "optimizer = AdamW(model.head_b.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY) # Head-BÎßå ÌïôÏäµ\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_epoch_coco(model, criterion, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    model.head_b.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"COCO Test Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss_dict = criterion(outputs, targets_gpu)\n",
    "            weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(weighted_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += weighted_loss.item()\n",
    "        pbar.set_postfix({'loss': f'{weighted_loss.item():.4f}'})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_coco(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar:\n",
    "            images = images.to(device)\n",
    "            targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss_dict = criterion(outputs, targets_gpu)\n",
    "                weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "            val_losses.append(weighted_loss.item())\n",
    "\n",
    "            # --- [ÏàòÏ†ï] Ïù¥ Î∂ÄÎ∂ÑÏù¥ ÏÉùÎûµÎêòÏóàÎçò Í≥≥ÏûÖÎãàÎã§ ---\n",
    "            # Î™®Îç∏Ïùò GPU Ï∂úÎ†•Í∞íÏùÑ CPUÎ°ú Í∞ÄÏ†∏Ïò¥\n",
    "            pred_logits_cpu = outputs['pred_logits'].cpu()\n",
    "            pred_masks_cpu = outputs['pred_masks'].cpu()\n",
    "            \n",
    "            # 1. PredictionÏùÑ torchmetrics ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "            preds_for_map = []\n",
    "            for i in range(len(targets)):\n",
    "                scores, labels = F.softmax(pred_logits_cpu[i], dim=-1).max(-1)\n",
    "                masks_bool = (torch.sigmoid(pred_masks_cpu[i]) > 0.5)\n",
    "                preds_for_map.append(dict(\n",
    "                    masks=masks_bool,\n",
    "                    scores=scores,\n",
    "                    labels=labels,\n",
    "                ))\n",
    "\n",
    "            # 2. TargetÏùÑ torchmetrics ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "            targets_for_map = []\n",
    "            for t in targets:\n",
    "                # CPUÎ°ú ÏòÆÍ∏∏ ÌïÑÏöî ÏóÜÏùå (targetsÎäî Ïù¥ÎØ∏ CPUÏóê ÏûàÏùå)\n",
    "                targets_for_map.append(dict(\n",
    "                    masks=(t['masks'] > 0.5), # boolean maskÎ°ú Î≥ÄÌôò\n",
    "                    labels=t['labels'],\n",
    "                ))\n",
    "            \n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "            \n",
    "    map_results = map_metric.compute()\n",
    "    return {'loss': np.mean(val_losses), 'mAP': map_results['map'].item()}\n",
    "\n",
    "print(\"‚úÖ COCO Test functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- üöÄ Starting COCO Sanity Check Training üöÄ ---\")\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_loss = train_epoch_coco(model, criterion, train_loader, optimizer, config.DEVICE, epoch)\n",
    "    val_metrics = validate_coco(model, criterion, val_loader, config.DEVICE)\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS} -> Train Loss: {train_loss:.4f}, Val Loss: {val_metrics['loss']:.4f}, mAP: {val_metrics['mAP']:.4f}\")\n",
    "\n",
    "print(\"\\n--- üéâ COCO Test Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f8a59",
   "metadata": {},
   "source": [
    "Mask R CNN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "‚úÖ COCO DataLoaders created!\n",
      "--- Mask R-CNN Test Configuration ---\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# --- pycocotoolsÏôÄ torchmetricsÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§ ---\n",
    "from pycocotools.coco import COCO\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, split='val'):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.images_dir = self.root / f\"{self.split}2017\"\n",
    "        self.coco = COCO(self.root / 'annotations' / f\"instances_{self.split}2017.json\")\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        self.cat_ids = sorted(self.coco.getCatIds())\n",
    "        self.coco_labels_map = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        coco_anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = self.images_dir / img_info['file_name']\n",
    "        \n",
    "        # --- [ÏàòÏ†ï] Ïù¥ÎØ∏ÏßÄÎ•º PIL Image Í∞ùÏ≤¥Î°ú Í∑∏ÎåÄÎ°ú Î∞òÌôò (transform Ï†úÍ±∞) ---\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        target = {}\n",
    "        masks, labels, boxes = [], [], []\n",
    "\n",
    "        for ann in coco_anns:\n",
    "            if ann['iscrowd'] == 0 and ann['area'] > 0:\n",
    "                masks.append(self.coco.annToMask(ann))\n",
    "                labels.append(self.coco_labels_map[ann['category_id']])\n",
    "                x, y, w, h = ann['bbox']\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "        \n",
    "        if boxes:\n",
    "            target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            target['labels'] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            target['masks'] = torch.as_tensor(np.stack(masks), dtype=torch.uint8)\n",
    "        else: \n",
    "            target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "            target['masks'] = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "# --- [ÏàòÏ†ï] collate_fnÏóêÏÑú ToTensor Î≥ÄÌôòÏùÑ Îã¥Îãπ ---\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    for item in batch:\n",
    "        # Í∞Å PIL Ïù¥ÎØ∏ÏßÄÎ•º ÌÖêÏÑúÎ°ú Î≥ÄÌôòÌïòÏó¨ Î¶¨Ïä§Ìä∏Ïóê Ï∂îÍ∞Ä\n",
    "        images.append(to_tensor(item[0]))\n",
    "        targets.append(item[1])\n",
    "    return images, targets\n",
    "\n",
    "# COCO val Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ train/valÎ°ú ÎÇòÎà†ÏÑú ÌÖåÏä§Ìä∏\n",
    "full_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "train_size = int(0.1 * len(full_dataset)) # 10%Îßå Îπ†Î•¥Í≤å ÌÖåÏä§Ìä∏\n",
    "val_size = int(0.02 * len(full_dataset)) # 2%Îßå Í≤ÄÏ¶ù\n",
    "train_subset, val_subset, _ = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, len(full_dataset) - train_size - val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "print(\"‚úÖ COCO DataLoaders created!\")\n",
    "\n",
    "class CocoConfig:\n",
    "    DATA_ROOT = Path('./data/coco')\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 2\n",
    "    LR = 5e-4 # Mask R-CNNÏùÄ ÏïΩÍ∞Ñ Îçî ÎÜíÏùÄ LRÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "config = CocoConfig()\n",
    "print(\"--- Mask R-CNN Test Configuration ---\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4491bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n",
      "‚úÖ COCO DataLoaders created!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ± (Ïù¥Ï†ÑÍ≥º ÎèôÏùº)\n",
    "# ==============================================================================\n",
    "full_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "train_size = int(0.1 * len(full_dataset))\n",
    "val_size = int(0.02 * len(full_dataset))\n",
    "train_subset, val_subset, _ = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, len(full_dataset) - train_size - val_size]\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "print(\"‚úÖ COCO DataLoaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1460ced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mask R-CNN model created!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. Mask R-CNN Î™®Îç∏ ÏÉùÏÑ±\n",
    "# ==============================================================================\n",
    "def get_maskrcnn_model(num_classes):\n",
    "    # COCOÏóêÏÑú ÏÇ¨Ï†Ñ ÌïôÏäµÎêú Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # Î∂ÑÎ•òÍ∏∞(classifier)Î•º ÏÉàÎ°úÏö¥ ÌÅ¥ÎûòÏä§ ÏàòÏóê ÎßûÍ≤å ÍµêÏ≤¥\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # ÎßàÏä§ÌÅ¨ ÏòàÏ∏°Í∏∞(mask predictor)Î•º ÏÉàÎ°úÏö¥ ÌÅ¥ÎûòÏä§ ÏàòÏóê ÎßûÍ≤å ÍµêÏ≤¥\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# COCOÎäî 80Í∞ú ÌÅ¥ÎûòÏä§ + Î∞∞Í≤Ω(background) 1Í∞ú = Ï¥ù 81Í∞ú\n",
    "model = get_maskrcnn_model(81).to(config.DEVICE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=config.LR)\n",
    "print(\"‚úÖ Mask R-CNN model created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a179671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COCO Test functions are ready.\n"
     ]
    }
   ],
   "source": [
    "def train_epoch_coco(model, criterion, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    model.head_b.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"COCO Test Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss_dict = criterion(outputs, targets_gpu)\n",
    "            weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(weighted_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += weighted_loss.item()\n",
    "        pbar.set_postfix({'loss': f'{weighted_loss.item():.4f}'})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_coco(model, dataloader, device):\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(dataloader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar_val:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            \n",
    "            # --- [ÌïµÏã¨ ÏàòÏ†ï] torchmetrics ÌòïÏãù Î≥ÄÌôò Ïãú uint8 ÏÇ¨Ïö© ---\n",
    "            preds_for_map = []\n",
    "            for p in predictions:\n",
    "                p_cpu = {k: v.cpu() for k, v in p.items()}\n",
    "                # ÏòàÏ∏°Îêú ÎßàÏä§ÌÅ¨Î•º thresholding ÌõÑ uint8Î°ú Î≥ÄÌôò\n",
    "                p_cpu['masks'] = (p_cpu['masks'].squeeze(1) > 0.5).to(torch.uint8)\n",
    "                preds_for_map.append(p_cpu)\n",
    "\n",
    "            targets_for_map = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "            \n",
    "    map_results = map_metric.compute()\n",
    "    print(f\"\\n -> Val mAP: {map_results['map'].item():.4f}\")\n",
    "\n",
    "print(\"‚úÖ COCO Test functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53de8802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üöÄ Starting Mask R-CNN Sanity Check Training üöÄ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mask R-CNN Epoch 1/2 [Train]:   0%|          | 0/125 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMask R-CNN Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Train]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- ÌïôÏäµ ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1175\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mcollate_fn\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     74\u001b[39m to_tensor = transforms.ToTensor()\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# Í∞Å PIL Ïù¥ÎØ∏ÏßÄÎ•º ÌÖêÏÑúÎ°ú Î≥ÄÌôòÌïòÏó¨ Î¶¨Ïä§Ìä∏Ïóê Ï∂îÍ∞Ä\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     images.append(\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     78\u001b[39m     targets.append(item[\u001b[32m1\u001b[39m])\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m images, targets\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:142\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    140\u001b[39m     _log_api_usage_once(to_tensor)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil._is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. Mask R-CNN ÌïôÏäµ Î£®ÌîÑ\n",
    "# ==============================================================================\n",
    "print(\"\\n--- üöÄ Starting Mask R-CNN Sanity Check Training üöÄ ---\")\n",
    "for epoch in range(config.EPOCHS):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Mask R-CNN Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    \n",
    "    # --- ÌïôÏäµ ---\n",
    "    for images, targets in pbar:\n",
    "        images = list(img.to(config.DEVICE) for img in images)\n",
    "        targets = [{k: v.to(config.DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Mask R-CNNÏùÄ ÌïôÏäµ Ïãú loss ÎîïÏÖîÎÑàÎ¶¨Î•º ÏßÅÏ†ë Î∞òÌôò\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{losses.item():.4f}'})\n",
    "\n",
    "    # --- Í≤ÄÏ¶ù ---\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(val_loader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar_val:\n",
    "            images = list(img.to(config.DEVICE) for img in images)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            \n",
    "            # torchmetrics ÌòïÏãùÏóê ÎßûÍ≤å Î≥ÄÌôò\n",
    "            preds_for_map = [{k: v.cpu() for k, v in p.items()} for p in predictions]\n",
    "            targets_for_map = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "            \n",
    "    map_results = map_metric.compute()\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS} -> mAP: {map_results['map'].item():.4f}\")\n",
    "\n",
    "print(\"\\n--- üéâ Mask R-CNN Test Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3639b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ Starting Final Pre-flight Check... ---\n",
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "‚úÖ Dataset and DataLoader are OK.\n",
      "‚úÖ Model initialization is OK.\n",
      "‚úÖ Training forward pass is OK (Loss: 5.8424).\n",
      "‚úÖ Validation logic and mAP calculation is OK (Sample mAP: 0.0000).\n",
      "\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ Pre-flight Check PASSED! All components are working correctly. ‚úÖ‚úÖ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# pycocotoolsÏôÄ torchmetricsÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.\n",
    "from pycocotools.coco import COCO\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "print(\"--- üöÄ Starting Final Pre-flight Check... ---\")\n",
    "\n",
    "try:\n",
    "    # ----------------------------------------------------\n",
    "    # 1. ÏÑ§Ï†ï (Configuration)\n",
    "    # ----------------------------------------------------\n",
    "    class CocoConfig:\n",
    "        DATA_ROOT = Path('./data/coco')\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        BATCH_SIZE = 2 # ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏûëÏùÄ Î∞∞Ïπò ÌÅ¨Í∏∞ ÏÇ¨Ïö©\n",
    "        NUM_WORKERS = 0\n",
    "\n",
    "    config = CocoConfig()\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2. Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ Î∞è Collate Ìï®Ïàò (Î™®Îì† ÏàòÏ†ïÏÇ¨Ìï≠ Î∞òÏòÅ)\n",
    "    # ----------------------------------------------------\n",
    "    class CocoDataset(Dataset):\n",
    "        def __init__(self, root, split='val'):\n",
    "            self.root = Path(root)\n",
    "            self.split = split\n",
    "            self.images_dir = self.root / f\"{split}2017\"\n",
    "            self.coco = COCO(self.root / 'annotations' / f\"instances_{split}2017.json\")\n",
    "            self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "            self.cat_ids = sorted(self.coco.getCatIds())\n",
    "            self.coco_labels_map = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}\n",
    "            self.transform = transforms.ToTensor()\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_id = self.ids[idx]\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            coco_anns = self.coco.loadAnns(ann_ids)\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            image = Image.open(self.images_dir / img_info['file_name']).convert('RGB')\n",
    "            \n",
    "            target = {}\n",
    "            masks, labels, boxes = [], [], []\n",
    "            for ann in coco_anns:\n",
    "                if ann['iscrowd'] == 0 and ann['area'] > 0:\n",
    "                    masks.append(self.coco.annToMask(ann))\n",
    "                    labels.append(self.coco_labels_map[ann['category_id']])\n",
    "                    x, y, w, h = ann['bbox']\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "            \n",
    "            image = self.transform(image)\n",
    "            \n",
    "            if boxes:\n",
    "                target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                target['labels'] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "                target['masks'] = torch.as_tensor(np.stack(masks), dtype=torch.uint8)\n",
    "            else: \n",
    "                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "                target['masks'] = torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "            return image, target\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3. Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ±\n",
    "    # ----------------------------------------------------\n",
    "    val_dataset = CocoDataset(root=config.DATA_ROOT, split='val')\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, collate_fn=collate_fn)\n",
    "    \n",
    "    print(\"‚úÖ Dataset and DataLoader are OK.\")\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # 4. Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "    # ----------------------------------------------------\n",
    "    def get_maskrcnn_model(num_classes):\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "        return model\n",
    "\n",
    "    model = get_maskrcnn_model(81).to(config.DEVICE)\n",
    "    print(\"‚úÖ Model initialization is OK.\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 5. ÏµúÏ¢Ö Í≤ÄÏ¶ù Î°úÏßÅ ÌÖåÏä§Ìä∏\n",
    "    # ----------------------------------------------------\n",
    "    images, targets = next(iter(val_loader))\n",
    "    \n",
    "    # ÌïôÏäµ Î™®Îìú ÌÖåÏä§Ìä∏\n",
    "    model.train()\n",
    "    images_gpu = list(img.to(config.DEVICE) for img in images)\n",
    "    targets_gpu = [{k: v.to(config.DEVICE) for k, v in t.items()} for t in targets]\n",
    "    loss_dict = model(images_gpu, targets_gpu)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    print(f\"‚úÖ Training forward pass is OK (Loss: {losses.item():.4f}).\")\n",
    "\n",
    "    # Í≤ÄÏ¶ù Î™®Îìú ÌÖåÏä§Ìä∏\n",
    "    model.eval()\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images_gpu)\n",
    "        preds_for_map = []\n",
    "        for p in predictions:\n",
    "            p_cpu = {k: v.cpu() for k, v in p.items()}\n",
    "            p_cpu['masks'] = (p_cpu['masks'].squeeze(1) > 0.5).to(torch.uint8)\n",
    "            preds_for_map.append(p_cpu)\n",
    "        targets_for_map = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "        map_metric.update(preds_for_map, targets_for_map)\n",
    "        map_results = map_metric.compute()\n",
    "\n",
    "    print(f\"‚úÖ Validation logic and mAP calculation is OK (Sample mAP: {map_results['map']:.4f}).\")\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ‚úÖ‚úÖ Pre-flight Check PASSED! All components are working correctly. ‚úÖ‚úÖ‚úÖ\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n‚ùå‚ùå‚ùå Pre-flight Check FAILED! An error occurred. ‚ùå‚ùå‚ùå\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de976b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd855c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
