{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557f2ff5",
   "metadata": {},
   "source": [
    "Hybrid_model : ConvNext-FPN Mask2Former Îßå ÏÇ¨Ïö©, Gaussian Distance ÏÇ¨Ïö©, mAP 0 Îú∏. F1, precision, recall ÏùÄ Ïò¨ÎùºÍ∞ê. Í∑ºÎç∞ Ï†ÑÌòÄ Î™ª ÎßûÏ∂∞ÏÑú Ïã§Ìå®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e49ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Master Dataset ---\n",
      "‚úÖ DataLoaders created!\n",
      "--- Initializing Hybrid Unified Model ---\n",
      "‚úÖ Initialization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_13772\\2379612393.py:134: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "\n",
    "# --- ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä Î∞è Î™®Îìà ÏûÑÌè¨Ìä∏ ---\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from models.hybrid_model import HybridUnifiedModel\n",
    "from utils.criterion import SetCriterion\n",
    "from utils.hungarian_matcher import HungarianMatcher\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from torchmetrics.classification import MulticlassJaccardIndex, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "# --- ÏµúÏ¢Ö ÏÑ§Ï†ï ---\n",
    "class FinalConfig:\n",
    "    DATA_ROOT = Path('./data/master_dataset')\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    EPOCHS = 50\n",
    "    LR = 3e-5\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 0\n",
    "    GRADIENT_CLIP_VAL = 1.0\n",
    "    WARMUP_EPOCHS = 5\n",
    "\n",
    "    MODEL = SimpleNamespace(\n",
    "        BACKBONE=SimpleNamespace(NAME='ConvNeXt-Tiny'),\n",
    "        FPN=SimpleNamespace(OUT_CHANNELS=256),\n",
    "        HEAD_B=SimpleNamespace(\n",
    "            FEAT_CHANNELS=256,\n",
    "            OUT_CHANNELS=256,\n",
    "            NUM_CLASSES=4,\n",
    "            QUERIES_PER_CLASS=75,\n",
    "            DEC_LAYERS=6\n",
    "        )\n",
    "    )\n",
    "    LOSS = SimpleNamespace(\n",
    "        CLASS_WEIGHTS=[0.5, 1.5, 1.0, 1.3],\n",
    "        EOS_COEF=0.1\n",
    "    )\n",
    "config = FinalConfig()\n",
    "\n",
    "# --- Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ Î∞è Î°úÎçî ---\n",
    "class UnifiedDataset(Dataset):\n",
    "    def __init__(self, image_infos, annotations_map, images_dir, transform=None):\n",
    "        self.image_infos = image_infos\n",
    "        self.annotations_map = annotations_map\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.cat_map = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_infos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.image_infos[idx]\n",
    "        img_id = img_info['id']\n",
    "        image = Image.open(self.images_dir / img_info['file_name']).convert('RGB')\n",
    "        original_w, original_h = image.size\n",
    "        \n",
    "        target = {}\n",
    "        masks, labels = [], []\n",
    "        annotations = self.annotations_map.get(img_id, [])\n",
    "        for ann in annotations:\n",
    "            if not ann.get('segmentation') or not ann['segmentation'][0] or len(ann['segmentation'][0]) < 6: continue\n",
    "            rle = mask_utils.frPyObjects([ann['segmentation'][0]], original_h, original_h)\n",
    "            mask = mask_utils.decode(rle)\n",
    "            if mask.ndim == 3: mask = np.max(mask, axis=2)\n",
    "            masks.append(mask)\n",
    "            labels.append(self.cat_map[ann['category_id']])\n",
    "\n",
    "        if self.transform: image = self.transform(image)\n",
    "        \n",
    "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # --- [ÌïµÏã¨ ÏàòÏ†ï] ÎßàÏä§ÌÅ¨ ÌÉÄÏûÖÏùÑ Îã§Ïãú float32Î°ú Î≥ÄÍ≤Ω ---\n",
    "        if masks:\n",
    "            target['masks'] = torch.from_numpy(np.stack(masks)).to(torch.float32)\n",
    "        else:\n",
    "            target['masks'] = torch.zeros((0, original_h, original_w), dtype=torch.float32)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch): return tuple(zip(*batch))\n",
    "\n",
    "# --- Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïã§Ìñâ ---\n",
    "print(\"--- Loading Master Dataset ---\")\n",
    "with open(config.DATA_ROOT / 'master_annotations.json', 'r') as f: master_data = json.load(f)\n",
    "images_info = master_data['images']\n",
    "annotations_map = {}\n",
    "for ann in master_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    if img_id not in annotations_map: annotations_map[img_id] = []\n",
    "    annotations_map[img_id].append(ann)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "full_dataset = UnifiedDataset(images_info, annotations_map, config.DATA_ROOT / 'images', transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, collate_fn=collate_fn)\n",
    "print(\"‚úÖ DataLoaders created!\")\n",
    "\n",
    "# --- Î™®Îç∏, ÏÜêÏã§Ìï®Ïàò, ÏòµÌã∞ÎßàÏù¥Ï†Ä Ï¥àÍ∏∞Ìôî ---\n",
    "print(\"--- Initializing Hybrid Unified Model ---\")\n",
    "model = HybridUnifiedModel(config).to(config.DEVICE)\n",
    "matcher = HungarianMatcher(num_classes=config.MODEL.HEAD_B.NUM_CLASSES)\n",
    "weight_dict = {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0}\n",
    "criterion = SetCriterion(\n",
    "    num_classes=config.MODEL.HEAD_B.NUM_CLASSES, matcher=matcher, weight_dict=weight_dict,\n",
    "    eos_coef=config.LOSS.EOS_COEF, losses=['labels', 'masks'], class_weights=config.LOSS.CLASS_WEIGHTS\n",
    ").to(config.DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "scaler = GradScaler()\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=config.WARMUP_EPOCHS)\n",
    "main_scheduler = CosineAnnealingLR(optimizer, T_max=config.EPOCHS - config.WARMUP_EPOCHS, eta_min=1e-7)\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[config.WARMUP_EPOCHS])\n",
    "print(\"‚úÖ Initialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45f71b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    for images, targets in pbar:\n",
    "        images = torch.stack(images).to(device)\n",
    "        targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss_dict = criterion(outputs, targets_gpu)\n",
    "            weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(weighted_loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP_VAL)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        pbar.set_postfix({'loss': f'{weighted_loss.item():.4f}'})\n",
    "\n",
    "def validate(model, criterion, dataloader, device):\n",
    "    \"\"\"\n",
    "    Î™®Îç∏ Í≤ÄÏ¶ù Ìï®Ïàò\n",
    "    Args:\n",
    "        model: ÌèâÍ∞ÄÌï† Î™®Îç∏\n",
    "        criterion: ÏÜêÏã§ Ìï®Ïàò (matcher Ï†ëÍ∑ºÏö©)\n",
    "        dataloader: Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "        device: ÎîîÎ∞îÏù¥Ïä§ (cuda/cpu)\n",
    "    Returns:\n",
    "        metrics: ÌèâÍ∞Ä ÏßÄÌëú ÎîïÏÖîÎÑàÎ¶¨\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # --- ÌèâÍ∞Ä ÏßÄÌëú Í∞ùÏ≤¥ Ï¥àÍ∏∞Ìôî ---\n",
    "    num_classes = config.MODEL.HEAD_B.NUM_CLASSES\n",
    "    \n",
    "    # 1. mAP (Detection/Segmentation ÌëúÏ§Ä)\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "    \n",
    "    # 2. IoU (Jaccard Index) - ÌÅ¥ÎûòÏä§Î≥Ñ ÌèâÍ∑†\n",
    "    iou_metric = MulticlassJaccardIndex(num_classes=num_classes, average='macro').to(device)\n",
    "    \n",
    "    # 3. Precision, Recall, F1-score - ÌÅ¥ÎûòÏä§Î≥Ñ ÌèâÍ∑†\n",
    "    precision_metric = MulticlassPrecision(num_classes=num_classes, average='macro').to(device)\n",
    "    recall_metric = MulticlassRecall(num_classes=num_classes, average='macro').to(device)\n",
    "    f1_metric = MulticlassF1Score(num_classes=num_classes, average='macro').to(device)\n",
    "    \n",
    "    # Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏ Ïó¨Î∂Ä Ï∂îÏ†Å\n",
    "    metrics_updated = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar:\n",
    "            images = torch.stack(images).to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "            \n",
    "            # --- mAP Í≥ÑÏÇ∞Ïö© Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ---\n",
    "            preds_cpu = {k: v.cpu() for k, v in outputs.items() if torch.is_tensor(v)}\n",
    "            \n",
    "            preds_for_map = []\n",
    "            for i in range(len(targets)):\n",
    "                scores, labels = F.softmax(preds_cpu['pred_logits'][i], dim=-1).max(-1)\n",
    "                # boolean ÎßàÏä§ÌÅ¨Î•º uint8Î°ú Î≥ÄÌôò\n",
    "                masks_uint8 = (torch.sigmoid(preds_cpu['pred_masks'][i]) > 0.5).to(torch.uint8)\n",
    "                preds_for_map.append(dict(masks=masks_uint8, scores=scores, labels=labels))\n",
    "\n",
    "            targets_for_map = []\n",
    "            for t in targets:\n",
    "                target_dict = {\n",
    "                    'labels': t['labels'].cpu(),\n",
    "                    # Ï†ïÎãµ ÎßàÏä§ÌÅ¨ÎèÑ uint8 ÌÉÄÏûÖÏúºÎ°ú Î≥ÄÌôò\n",
    "                    'masks': t['masks'].cpu().to(torch.uint8)\n",
    "                }\n",
    "                targets_for_map.append(target_dict)\n",
    "\n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "            \n",
    "            # --- IoU, P, R, F1 Í≥ÑÏÇ∞Ïö© Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ---\n",
    "            indices = criterion.matcher(outputs, [{k: v.to(device) for k, v in t.items()} for t in targets])\n",
    "            \n",
    "            if any(len(i[0]) > 0 for i in indices):\n",
    "                idx = criterion._get_src_permutation_idx(indices)\n",
    "                pred_logits = outputs['pred_logits'][idx]\n",
    "                target_labels = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)]).to(device)\n",
    "\n",
    "                # Îß§Ïπ≠Îêú Í≤∞Í≥ºÎ°ú ÏßÄÌëú ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                iou_metric.update(pred_logits, target_labels)\n",
    "                precision_metric.update(pred_logits, target_labels)\n",
    "                recall_metric.update(pred_logits, target_labels)\n",
    "                f1_metric.update(pred_logits, target_labels)\n",
    "                metrics_updated = True\n",
    "\n",
    "    # ÏµúÏ¢Ö Í≤∞Í≥º ÏßëÍ≥Ñ\n",
    "    try:\n",
    "        map_results = map_metric.compute()\n",
    "        map_val = map_results.get('map', torch.tensor(0.0))\n",
    "        map_50_val = map_results.get('map_50', torch.tensor(0.0))\n",
    "        \n",
    "        # tensorÏù∏ÏßÄ ÌôïÏù∏ÌïòÍ≥† item() Ìò∏Ï∂ú\n",
    "        if torch.is_tensor(map_val):\n",
    "            map_val = map_val.item()\n",
    "        if torch.is_tensor(map_50_val):\n",
    "            map_50_val = map_50_val.item()\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Warning: mAP computation failed\")\n",
    "        map_val = 0.0\n",
    "        map_50_val = 0.0\n",
    "    \n",
    "    # Îã§Î•∏ Î©îÌä∏Î¶≠Îì§ Í≥ÑÏÇ∞\n",
    "    if metrics_updated:\n",
    "        try:\n",
    "            iou_val = iou_metric.compute().item()\n",
    "        except:\n",
    "            iou_val = 0.0\n",
    "        \n",
    "        try:\n",
    "            precision_val = precision_metric.compute().item()\n",
    "        except:\n",
    "            precision_val = 0.0\n",
    "            \n",
    "        try:\n",
    "            recall_val = recall_metric.compute().item()\n",
    "        except:\n",
    "            recall_val = 0.0\n",
    "            \n",
    "        try:\n",
    "            f1_val = f1_metric.compute().item()\n",
    "        except:\n",
    "            f1_val = 0.0\n",
    "    else:\n",
    "        # Î©îÌä∏Î¶≠Ïù¥ Ìïú Î≤àÎèÑ ÏóÖÎç∞Ïù¥Ìä∏ÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞\n",
    "        print(\"‚ö†Ô∏è Warning: No valid predictions matched with targets\")\n",
    "        iou_val = precision_val = recall_val = f1_val = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'mAP': map_val,\n",
    "        'mAP_50': map_50_val,\n",
    "        'iou': iou_val,\n",
    "        'precision': precision_val,\n",
    "        'recall': recall_val,\n",
    "        'f1_score': f1_val\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "736974b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üöÄ Starting Hybrid Unified Model Training üöÄ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 0/655 [00:00<?, ?it/s]C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_13772\\1293238820.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=6.2026] \n",
      "[Valid]:   0%|          | 0/164 [00:00<?, ?it/s]C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_13772\\1293238820.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2091 | F1: 0.3120\n",
      "  Precision: 0.5245 | Recall: 0.3580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:44<00:00,  1.62it/s, loss=4.4183]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:27<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.1826 | F1: 0.2692\n",
      "  Precision: 0.5048 | Recall: 0.4049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:50<00:00,  1.59it/s, loss=5.5379] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.1881 | F1: 0.2836\n",
      "  Precision: 0.4199 | Recall: 0.3226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=7.5676] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2325 | F1: 0.3702\n",
      "  Precision: 0.3975 | Recall: 0.4046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=7.3938] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n",
      "c:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2449 | F1: 0.3594\n",
      "  Precision: 0.3797 | Recall: 0.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=5.6607] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2083 | F1: 0.3375\n",
      "  Precision: 0.4139 | Recall: 0.3514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=7.6219] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.1485 | F1: 0.2508\n",
      "  Precision: 0.2549 | Recall: 0.2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.64it/s, loss=1.6495] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.5350 | F1: 0.6613\n",
      "  Precision: 0.7152 | Recall: 0.6734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=8.3342] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2372 | F1: 0.3705\n",
      "  Precision: 0.3753 | Recall: 0.3774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=8.4895] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.1817 | F1: 0.2878\n",
      "  Precision: 0.3134 | Recall: 0.3108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=6.6898] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2125 | F1: 0.3346\n",
      "  Precision: 0.3524 | Recall: 0.3694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.64it/s, loss=6.8351]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2624 | F1: 0.4101\n",
      "  Precision: 0.4102 | Recall: 0.4305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=3.0545] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.4192 | F1: 0.5474\n",
      "  Precision: 0.5901 | Recall: 0.5601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=5.5454] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.3298 | F1: 0.4791\n",
      "  Precision: 0.5163 | Recall: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.64it/s, loss=4.6245] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2632 | F1: 0.3969\n",
      "  Precision: 0.4319 | Recall: 0.4411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=3.5817] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2821 | F1: 0.4246\n",
      "  Precision: 0.4830 | Recall: 0.4619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.64it/s, loss=5.5957] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.3289 | F1: 0.4791\n",
      "  Precision: 0.5083 | Recall: 0.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=6.3058] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2441 | F1: 0.3887\n",
      "  Precision: 0.3833 | Recall: 0.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=6.4287]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2672 | F1: 0.4094\n",
      "  Precision: 0.4222 | Recall: 0.4047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=1.8159]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.3709 | F1: 0.5085\n",
      "  Precision: 0.5606 | Recall: 0.4992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=3.1577] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2017 | F1: 0.3039\n",
      "  Precision: 0.4411 | Recall: 0.3209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=6.1988] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2004 | F1: 0.3184\n",
      "  Precision: 0.3700 | Recall: 0.3254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=5.7442] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2610 | F1: 0.4028\n",
      "  Precision: 0.4508 | Recall: 0.3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.64it/s, loss=7.8434] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.4425 | F1: 0.6028\n",
      "  Precision: 0.6265 | Recall: 0.5929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.64it/s, loss=4.1192] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.4028 | F1: 0.5536\n",
      "  Precision: 0.5901 | Recall: 0.5553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:40<00:00,  1.63it/s, loss=7.1493]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.2344 | F1: 0.3619\n",
      "  Precision: 0.4128 | Recall: 0.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=2.1421] \n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.5240 | F1: 0.6627\n",
      "  Precision: 0.6980 | Recall: 0.6544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=1.2596]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.6021 | F1: 0.7278\n",
      "  Precision: 0.7288 | Recall: 0.7386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=3.8005]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.5740 | F1: 0.6938\n",
      "  Precision: 0.6882 | Recall: 0.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:42<00:00,  1.63it/s, loss=2.6286]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7203 | F1: 0.8270\n",
      "  Precision: 0.8210 | Recall: 0.8349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=2.0859]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7126 | F1: 0.8214\n",
      "  Precision: 0.8566 | Recall: 0.8001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=0.7915]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7115 | F1: 0.8187\n",
      "  Precision: 0.8352 | Recall: 0.8067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=2.3643]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.6853 | F1: 0.7962\n",
      "  Precision: 0.8456 | Recall: 0.7789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:42<00:00,  1.63it/s, loss=1.2871]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7036 | F1: 0.8082\n",
      "  Precision: 0.8777 | Recall: 0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=0.6665]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7083 | F1: 0.8156\n",
      "  Precision: 0.8578 | Recall: 0.8027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=0.7545]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7415 | F1: 0.8422\n",
      "  Precision: 0.8593 | Recall: 0.8314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=1.2581]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.6948 | F1: 0.8039\n",
      "  Precision: 0.8300 | Recall: 0.7882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=4.5561]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7011 | F1: 0.8083\n",
      "  Precision: 0.8569 | Recall: 0.7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=2.3546]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7302 | F1: 0.8338\n",
      "  Precision: 0.8909 | Recall: 0.8082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=1.2932]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7032 | F1: 0.8124\n",
      "  Precision: 0.8338 | Recall: 0.7976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=2.6223]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7305 | F1: 0.8330\n",
      "  Precision: 0.8736 | Recall: 0.8115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=1.3673]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7534 | F1: 0.8522\n",
      "  Precision: 0.8617 | Recall: 0.8456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=2.4409]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7506 | F1: 0.8502\n",
      "  Precision: 0.8616 | Recall: 0.8430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=2.1028]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7722 | F1: 0.8658\n",
      "  Precision: 0.8782 | Recall: 0.8561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:44<00:00,  1.62it/s, loss=3.0336]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7540 | F1: 0.8509\n",
      "  Precision: 0.8600 | Recall: 0.8441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:42<00:00,  1.63it/s, loss=1.1953]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:27<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7244 | F1: 0.8313\n",
      "  Precision: 0.8324 | Recall: 0.8304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=1.1622]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7392 | F1: 0.8415\n",
      "  Precision: 0.8538 | Recall: 0.8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=3.0448]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7470 | F1: 0.8474\n",
      "  Precision: 0.8647 | Recall: 0.8369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=1.6188]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7545 | F1: 0.8522\n",
      "  Precision: 0.8670 | Recall: 0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655/655 [06:41<00:00,  1.63it/s, loss=0.5522]\n",
      "[Valid]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:26<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50:\n",
      "  Val mAP: 0.0000 | mAP@.50: 0.0000\n",
      "  IoU: 0.7533 | F1: 0.8514\n",
      "  Precision: 0.8700 | Recall: 0.8407\n",
      "\n",
      "--- üéâ Training Complete ---\n",
      "Best mAP achieved: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- Î©îÏù∏ ÌïôÏäµ Î£®ÌîÑ (ÏôÑÏ†ÑÌïú Î≤ÑÏ†Ñ) ---\n",
    "print(\"\\n--- üöÄ Starting Hybrid Unified Model Training üöÄ ---\")\n",
    "best_map = 0.0\n",
    "for epoch in range(config.EPOCHS):\n",
    "    # ÌïôÏäµ\n",
    "    train_epoch(model, criterion, train_loader, optimizer, config.DEVICE, epoch)\n",
    "    \n",
    "    # Í≤ÄÏ¶ù - criterionÎèÑ Ï†ÑÎã¨\n",
    "    val_metrics = validate(model, criterion, val_loader, config.DEVICE)\n",
    "    \n",
    "    # Î©îÌä∏Î¶≠ Ï∂úÎ†•\n",
    "    val_map = val_metrics['mAP']\n",
    "    val_map_50 = val_metrics['mAP_50']\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}:\")\n",
    "    print(f\"  Val mAP: {val_map:.4f} | mAP@.50: {val_map_50:.4f}\")\n",
    "    print(f\"  IoU: {val_metrics['iou']:.4f} | F1: {val_metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f}\")\n",
    "    \n",
    "    # Ïä§ÏºÄÏ§ÑÎü¨ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Î≤†Ïä§Ìä∏ Î™®Îç∏ Ï†ÄÏû•\n",
    "    if val_map > best_map:\n",
    "        best_map = val_map\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_map': best_map,\n",
    "            'metrics': val_metrics\n",
    "        }, 'hybrid_unified_best_model.pth')\n",
    "        print(f\"‚ú® New best model saved with mAP: {best_map:.4f}\")\n",
    "        \n",
    "print(f\"\\n--- üéâ Training Complete ---\")\n",
    "print(f\"Best mAP achieved: {best_map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e499dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ Starting Single Batch Overfit Test ---\n",
      "‚úÖ Single batch loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_13772\\547243036.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_test = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting to overfit on the single batch for 200 iterations ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_13772\\547243036.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "  5%|‚ñå         | 10/200 [00:05<01:48,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/200 -> Loss: 10.7047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 20/200 [00:11<01:42,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20/200 -> Loss: 9.3961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 30/200 [00:17<01:36,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30/200 -> Loss: 8.4513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 40/200 [00:22<01:30,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40/200 -> Loss: 9.1292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 50/200 [00:28<01:24,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50/200 -> Loss: 9.2957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 60/200 [00:34<01:19,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60/200 -> Loss: 7.6857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [00:39<01:13,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70/200 -> Loss: 7.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [00:45<01:08,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80/200 -> Loss: 8.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [00:51<01:03,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90/200 -> Loss: 8.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [00:57<00:58,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/200 -> Loss: 7.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [01:02<00:51,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110/200 -> Loss: 7.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [01:08<00:45,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 120/200 -> Loss: 8.4996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [01:14<00:39,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130/200 -> Loss: 7.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [01:19<00:34,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 140/200 -> Loss: 6.9739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [01:25<00:28,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150/200 -> Loss: 7.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [01:31<00:22,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 160/200 -> Loss: 8.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [01:36<00:17,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 170/200 -> Loss: 7.3558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [01:42<00:11,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 180/200 -> Loss: 7.2570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [01:48<00:05,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190/200 -> Loss: 8.6877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/200 -> Loss: 8.6085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- üöÄ Starting Single Batch Overfit Test ---\")\n",
    "\n",
    "# 1. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎçîÏóêÏÑú Îî± Ìïú Í∞úÏùò Î∞∞ÏπòÎßå Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "try:\n",
    "    single_batch = next(iter(train_loader))\n",
    "    print(\"‚úÖ Single batch loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load a batch: {e}\")\n",
    "\n",
    "if 'single_batch' in locals():\n",
    "    images, targets = single_batch\n",
    "    images = torch.stack(images).to(config.DEVICE)\n",
    "    targets_gpu = [{k: v.to(config.DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # 2. Î™®Îç∏Í≥º ÏòµÌã∞ÎßàÏù¥Ï†ÄÎ•º ÏÉàÎ°ú Ï¥àÍ∏∞Ìôî (Íπ®ÎÅóÌïú ÏÉÅÌÉúÏóêÏÑú ÏãúÏûë)\n",
    "    model_test = HybridUnifiedModel(config).to(config.DEVICE)\n",
    "    optimizer_test = AdamW(model_test.parameters(), lr=config.LR)\n",
    "    criterion_test = criterion.to(config.DEVICE)\n",
    "    scaler_test = GradScaler()\n",
    "    model_test.train()\n",
    "    \n",
    "    print(\"\\n--- Starting to overfit on the single batch for 200 iterations ---\")\n",
    "    # 3. ÎèôÏùºÌïú Î∞∞ÏπòÎ°ú 200Î≤à ÌïôÏäµ ÏãúÎèÑ\n",
    "    for i in tqdm(range(200)):\n",
    "        with autocast():\n",
    "            outputs = model_test(images)\n",
    "            loss_dict = criterion_test(outputs, targets_gpu)\n",
    "            weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        optimizer_test.zero_grad()\n",
    "        scaler_test.scale(weighted_loss).backward()\n",
    "        scaler_test.step(optimizer_test)\n",
    "        scaler_test.update()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Iteration {i+1}/200 -> Loss: {weighted_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca36438",
   "metadata": {},
   "source": [
    "Unified_model : ConvNext-FPN Mask2Former Îßå ÏÇ¨Ïö©, Gaussian Distance ÏÇ¨Ïö©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b074679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading the best model and visualizing predictions ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_13772\\2759810527.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('hybrid_unified_best_model.pth'))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hybrid_unified_best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Loading the best model and visualizing predictions ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. ÏµúÏ¢Ö Î™®Îç∏ Í∞ÄÏ§ëÏπò Î∂àÎü¨Ïò§Í∏∞\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhybrid_unified_best_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2. ÏãúÍ∞ÅÌôî Ìï®Ïàò Ïã§Ìñâ\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# val_loaderÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏°ÏùÑ ÌôïÏù∏Ìï©ÎãàÎã§.\u001b[39;00m\n\u001b[32m     10\u001b[39m visualize_predictions(model, val_loader, config.DEVICE, num_samples=\u001b[32m5\u001b[39m) \u001b[38;5;66;03m# 5Í∞ú ÏÉòÌîå ÌôïÏù∏\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\yoloEnv\\Lib\\site-packages\\torch\\serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'hybrid_unified_best_model.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "\n",
    "# --- ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä Î∞è Î™®Îìà ÏûÑÌè¨Ìä∏ ---\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from models.unified_model import UnifiedModel\n",
    "from utils.criterion import SetCriterion\n",
    "from utils.hungarian_matcher import HungarianMatcher\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from torchmetrics.classification import MulticlassJaccardIndex, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "print(\"--- üöÄ Starting Single Batch Overfit Test (Simple Model) ---\")\n",
    "\n",
    "# 1. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎçîÏóêÏÑú Îî± Ìïú Í∞úÏùò Î∞∞ÏπòÎßå Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "try:\n",
    "    single_batch = next(iter(train_loader))\n",
    "    print(\"‚úÖ Single batch loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load a batch: {e}\")\n",
    "\n",
    "if 'single_batch' in locals():\n",
    "    images, targets = single_batch\n",
    "    images = torch.stack(images).to(config.DEVICE)\n",
    "    targets_gpu = [{k: v.to(config.DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # 2. Î™®Îç∏Í≥º ÏòµÌã∞ÎßàÏù¥Ï†ÄÎ•º ÏÉàÎ°ú Ï¥àÍ∏∞Ìôî (Íπ®ÎÅóÌïú ÏÉÅÌÉúÏóêÏÑú ÏãúÏûë)\n",
    "    model_test = UnifiedModel(config).to(config.DEVICE)\n",
    "    optimizer_test = AdamW(model_test.parameters(), lr=config.LR)\n",
    "    criterion_test = criterion.to(config.DEVICE)\n",
    "    scaler_test = GradScaler()\n",
    "    model_test.train()\n",
    "    \n",
    "    print(\"\\n--- Starting to overfit on the single batch for 200 iterations ---\")\n",
    "    # 3. ÎèôÏùºÌïú Î∞∞ÏπòÎ°ú 200Î≤à ÌïôÏäµ ÏãúÎèÑ\n",
    "    for i in tqdm(range(200)):\n",
    "        with autocast():\n",
    "            outputs = model_test(images)\n",
    "            loss_dict = criterion_test(outputs, targets_gpu)\n",
    "            weighted_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        optimizer_test.zero_grad()\n",
    "        scaler_test.scale(weighted_loss).backward()\n",
    "        scaler_test.step(optimizer_test)\n",
    "        scaler_test.update()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Iteration {i+1}/200 -> Loss: {weighted_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90739a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
