{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2640b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# pycocotoolsê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install pycocotools\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "# --- í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€ ë° ëª¨ë“ˆ ì„í¬íŠ¸ ---\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from models.blade_model_v2 import BladeModelV2\n",
    "from utils.criterion import SetCriterion\n",
    "from utils.hungarian_matcher import HungarianMatcher\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# --- ìµœì¢… ì„¤ì • (Configuration) ---\n",
    "class Config:\n",
    "    DATA_ROOT = Path('C:/EngineBladeAI/EngineInspectionAI_MS/data/final_dataset_augmented')\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # --- í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ìˆ˜ì •) ---\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 50\n",
    "    LR = 2e-5  # <-- [ìˆ˜ì •] 1e-4ëŠ” ë„ˆë¬´ ë†’ì•˜ìœ¼ë¯€ë¡œ, 2e-5 (0.00002)ë¡œ ë‚®ì¶¥ë‹ˆë‹¤.\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    NUM_WORKERS = 0\n",
    "    LR_DROP_STEP = 20\n",
    "    GRADIENT_CLIP_VAL = 1.0 # <-- [ì¶”ê°€] Gradient Clipping ê°’ ì„¤ì •\n",
    "\n",
    "    MODEL = SimpleNamespace(\n",
    "        # --- [ìˆ˜ì •] ë”•ì…”ë„ˆë¦¬ë¥¼ SimpleNamespaceë¡œ ë³€ê²½ ---\n",
    "        BACKBONE=SimpleNamespace(NAME='ConvNeXt-Tiny'),\n",
    "        FPN=SimpleNamespace(OUT_CHANNELS=256),\n",
    "        HEAD_B=SimpleNamespace(\n",
    "            FEAT_CHANNELS=256,\n",
    "            OUT_CHANNELS=256,\n",
    "            NUM_CLASSES=3,\n",
    "            QUERIES_PER_CLASS=100,\n",
    "            DEC_LAYERS=6\n",
    "        )\n",
    "    )\n",
    "    LOSS = SimpleNamespace(\n",
    "        CLASS_WEIGHTS=[1.5, 1.0, 1.3], # Crack, Nick, Tear\n",
    "        EOS_COEF=0.1\n",
    "    )\n",
    "\n",
    "config = Config()\n",
    "print(f\"\\n--- Configuration Initialized ---\")\n",
    "print(f\"Data Path: {config.DATA_ROOT}\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(f\"Initial Learning Rate: {config.LR}\") # <-- í™•ì¸ìš© print ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalBladeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ìµœì¢… í†µí•©ëœ ë°ì´í„°ì…‹(final_dataset)ì„ ìœ„í•œ Dataset í´ë˜ìŠ¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, split='train', transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.images_dir = self.root / self.split / 'images'\n",
    "        \n",
    "        json_path = self.root / self.split / 'annotations.json'\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            \n",
    "        self.images_info = self.data['images']\n",
    "        self.annotations_map = {}\n",
    "        for ann in self.data['annotations']:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.annotations_map:\n",
    "                self.annotations_map[img_id] = []\n",
    "            self.annotations_map[img_id].append(ann)\n",
    "            \n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((640, 640)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images_info[idx]\n",
    "        img_id = img_info['id']\n",
    "        img_path = self.images_dir / img_info['file_name']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        original_w, original_h = image.size\n",
    "        \n",
    "        target = {}\n",
    "        blade_mask = np.zeros((original_h, original_w), dtype=np.uint8)\n",
    "        damage_masks_np = []\n",
    "        damage_labels = []\n",
    "        multilabel_vector = torch.zeros(3, dtype=torch.float32)\n",
    "\n",
    "        annotations = self.annotations_map.get(img_id, [])\n",
    "        for ann in annotations:\n",
    "            # --- [í•µì‹¬ ìˆ˜ì •] ---\n",
    "            # segmentation ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ëŠ” ë°©ì–´ ì½”ë“œ ì¶”ê°€\n",
    "            seg = ann.get('segmentation')\n",
    "            if not seg or not isinstance(seg, list) or not seg[0] or len(seg[0]) < 6:\n",
    "                # ìœ íš¨í•˜ì§€ ì•Šì€ polygon (ìµœì†Œ 3ê°œì˜ ì  í•„ìš”)ì´ë©´ ê±´ë„ˆë›°ê¸°\n",
    "                continue\n",
    "                \n",
    "            cat_id = ann['category_id']\n",
    "            \n",
    "            try:\n",
    "                rle = mask_utils.frPyObjects([seg[0]], original_h, original_w)\n",
    "                mask = mask_utils.decode(rle)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to decode segmentation for ann_id {ann.get('id')}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            if mask.ndim == 3:\n",
    "                mask = np.max(mask, axis=2)\n",
    "\n",
    "            if cat_id == 1:\n",
    "                blade_mask = np.maximum(blade_mask, mask)\n",
    "            else:\n",
    "                damage_masks_np.append(mask)\n",
    "                damage_labels.append(cat_id - 2)\n",
    "                multilabel_vector[cat_id - 2] = 1.0\n",
    "\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        target['blade_mask'] = torch.from_numpy(blade_mask).long()\n",
    "        target['labels'] = torch.tensor(damage_labels, dtype=torch.int64)\n",
    "        target['multilabel'] = multilabel_vector\n",
    "        \n",
    "        if damage_masks_np:\n",
    "            damage_masks_tensor = torch.from_numpy(np.stack(damage_masks_np)).float()\n",
    "            target['masks'] = damage_masks_tensor\n",
    "        else:\n",
    "            target['masks'] = torch.zeros((0, original_h, original_w), dtype=torch.float32)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "print(\"âœ… Dataset class and collate_fn are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Creating DataLoaders ---\")\n",
    "train_dataset = FinalBladeDataset(root=config.DATA_ROOT, split='train')\n",
    "val_dataset = FinalBladeDataset(root=config.DATA_ROOT, split='valid')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "print(f\"âœ… DataLoaders created!\")\n",
    "print(f\"   Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3731d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ì…€ 4: ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” (ìˆ˜ì •) =====\n",
    "\n",
    "print(\"--- Initializing Model, Criterion, Optimizer ---\")\n",
    "model = BladeModelV2(config).to(config.DEVICE)\n",
    "\n",
    "matcher = HungarianMatcher(cost_class=2.0, cost_mask=5.0, cost_dice=5.0)\n",
    "# ìˆ˜ì •ëœ weight_dict (ì†ìƒ íƒì§€ì˜ ì¤‘ìš”ë„ë¥¼ í¬ê²Œ ë†’ì„)\n",
    "weight_dict = {'loss_ce': 5.0, 'loss_mask': 10.0, 'loss_dice': 10.0}\n",
    "\n",
    "criterion = SetCriterion(\n",
    "    num_classes=config.MODEL.HEAD_B.NUM_CLASSES, matcher=matcher, weight_dict=weight_dict,\n",
    "    eos_coef=config.LOSS.EOS_COEF, losses=['labels', 'masks'],\n",
    "    class_weights=config.LOSS.CLASS_WEIGHTS\n",
    ").to(config.DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# --- [ìˆ˜ì •] í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ---\n",
    "lr_scheduler = StepLR(optimizer, step_size=config.LR_DROP_STEP)\n",
    "\n",
    "print(\"âœ… Initialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae72ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchmetricsì—ì„œ í•„ìš”í•œ ëª¨ë“  í‰ê°€ ì§€í‘œ í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from torchmetrics.classification import MulticlassJaccardIndex # Blade IoUìš©\n",
    "\n",
    "\n",
    "# train_epoch í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.\n",
    "def train_epoch(model, criterion, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    total_loss = 0\n",
    "    blade_loss_weight = 1.0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.EPOCHS} [Train]\")\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss_dict = criterion(outputs, targets_gpu)\n",
    "            damage_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "            blade_logits = outputs['blade_logits']\n",
    "            gt_blade_masks = torch.stack([t['blade_mask'] for t in targets_gpu]).unsqueeze(1).float()\n",
    "            blade_logits_resized = F.interpolate(blade_logits, size=gt_blade_masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            loss_blade = F.binary_cross_entropy_with_logits(blade_logits_resized, gt_blade_masks)\n",
    "            weighted_loss = damage_loss + (loss_blade * blade_loss_weight)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(weighted_loss).backward()\n",
    "        \n",
    "        # --- [ì¶”ê°€] Gradient Clipping ---\n",
    "        # scalerê°€ unscaleì„ í•œ í›„ì— clippingì„ ì ìš©í•´ì•¼ í•¨\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP_VAL)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += weighted_loss.item()\n",
    "        pbar.set_postfix({'loss': f'{weighted_loss.item():.4f}', 'L_dmg': f'{damage_loss.item():.2f}', 'L_bld': f'{loss_blade.item():.2f}'})\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    \n",
    "    # --- [ìµœì¢… ìˆ˜ì •] Detection ì „ìš© í‰ê°€ ì§€í‘œ ê°ì²´ ì´ˆê¸°í™” ---\n",
    "    num_damage_classes = config.MODEL.HEAD_B.NUM_CLASSES\n",
    "    # 1. Blade IoU (ì´ì „ê³¼ ë™ì¼)\n",
    "    blade_iou_metric = MulticlassJaccardIndex(num_classes=2).to(device)\n",
    "    # 2. Damage mAP (íƒì§€ ë¬¸ì œì˜ í‘œì¤€ í‰ê°€ ì§€í‘œ)\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"segm\")\n",
    "\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"[Valid]\")\n",
    "        for images, targets in pbar:\n",
    "            images = images.to(device)\n",
    "            targets_gpu = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                # --- ì†ì‹¤ ê³„ì‚° (ê¸°ì¡´ê³¼ ë™ì¼) ---\n",
    "                loss_dict = criterion(outputs, targets_gpu)\n",
    "                damage_loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "                blade_logits = outputs['blade_logits']\n",
    "                gt_blade_masks = torch.stack([t['blade_mask'] for t in targets_gpu]).unsqueeze(1).float()\n",
    "                blade_logits_resized = F.interpolate(blade_logits, size=gt_blade_masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                loss_blade = F.binary_cross_entropy_with_logits(blade_logits_resized, gt_blade_masks)\n",
    "                weighted_loss = damage_loss + (loss_blade * 1.0)\n",
    "            val_losses.append(weighted_loss.item())\n",
    "            \n",
    "            # --- [ìµœì¢… ìˆ˜ì •] í‰ê°€ ì§€í‘œ ì—…ë°ì´íŠ¸ ---\n",
    "            # 1. Blade IoU\n",
    "            pred_blade_masks = (torch.sigmoid(blade_logits_resized) > 0.5).int().squeeze(1)\n",
    "            blade_iou_metric.update(pred_blade_masks, gt_blade_masks.squeeze(1).int())\n",
    "\n",
    "            # 2. mAP ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° í˜•ì‹ ë³€í™˜\n",
    "            pred_logits = outputs['pred_logits'].cpu()\n",
    "            pred_masks = outputs['pred_masks'].cpu()\n",
    "            \n",
    "            preds_for_map = []\n",
    "            for i in range(len(targets)):\n",
    "                scores, labels = F.softmax(pred_logits[i], dim=-1).max(-1)\n",
    "                masks_bool = (torch.sigmoid(pred_masks[i]) > 0.5)\n",
    "                \n",
    "                preds_for_map.append(dict(\n",
    "                    masks=masks_bool, scores=scores, labels=labels,\n",
    "                ))\n",
    "\n",
    "            targets_for_map = []\n",
    "            for t in targets:\n",
    "                targets_for_map.append(dict(\n",
    "                    masks=(t['masks'] > 0.5), labels=t['labels'],\n",
    "                ))\n",
    "            \n",
    "            map_metric.update(preds_for_map, targets_for_map)\n",
    "\n",
    "    # --- [ìµœì¢… ìˆ˜ì •] ëª¨ë“  ì§€í‘œ ê³„ì‚° ë° ì§‘ê³„ ---\n",
    "    blade_iou = blade_iou_metric.compute().item()\n",
    "    map_results = map_metric.compute()\n",
    "    \n",
    "    # mAP ê²°ê³¼ì—ì„œ Precision, Recall ì¶”ì¶œ\n",
    "    precision = map_results['map_50'].item() # mAP@50ì€ Precision-Recall ê³¡ì„ ì˜ ë©´ì \n",
    "    recall = map_results['mar_100'].item() # 100ê°œ ì˜ˆì¸¡ ì‹œ í‰ê·  ì¬í˜„ìœ¨\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': np.mean(val_losses),\n",
    "        'blade_iou': blade_iou,\n",
    "        'mAP': map_results['map'].item(),\n",
    "        'precision': precision, # ê·¼ì‚¬ì¹˜\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbab2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ì…€ 6ì˜ ë©”ì¸ í•™ìŠµ ë£¨í”„ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´ =====\n",
    "\n",
    "print(\"\\n--- ğŸš€ Starting Final Training ğŸš€ ---\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_loss = train_epoch(model, criterion, train_loader, optimizer, config.DEVICE, epoch)\n",
    "    val_metrics = validate(model, criterion, val_loader, config.DEVICE)\n",
    "    \n",
    "    val_loss = val_metrics['loss']\n",
    "    \n",
    "    # --- [ìµœì¢… ìˆ˜ì •] ìƒˆë¡œìš´ ì§€í‘œë“¤ ì¶œë ¥ ---\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS} -> Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  [Blade] IoU: {val_metrics['blade_iou']:.4f}\")\n",
    "    print(f\"  [Damage] mAP: {val_metrics['mAP']:.4f} | Precision: {val_metrics['precision']:.4f} | \"\n",
    "          f\"Recall: {val_metrics['recall']:.4f} | F1: {val_metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'blade_damage_best_model.pth')\n",
    "        print(f\"âœ¨ New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- ğŸ‰ Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06d87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
